{
  "metadata": {
    "name": "work_location_borough",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport sys\nfrom operator import add\nfrom pyspark import SparkContext\nfrom csv import reader\nimport sys\nfrom operator import add\nfrom pyspark import SparkContext\nfrom csv import reader"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndataset1 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.k397-673e.csv\").mapPartitions(lambda x: reader(x))\ndataset2 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.ic3t-wcy2.csv\").mapPartitions(lambda x: reader(x))\ndataset3 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.rbx6-tga4.csv\").mapPartitions(lambda x: reader(x))\ndataset4 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.vz8c-29aj.csv\").mapPartitions(lambda x: reader(x))\ndataset5 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.6khm-nrue.csv\").mapPartitions(lambda x: reader(x))\ndataset6 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.avir-tzek.csv\").mapPartitions(lambda x: reader(x))\ndataset7 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.xywu-7bv9.csv\").mapPartitions(lambda x: reader(x))\ndataset8 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.3qem-6v3v.csv\").mapPartitions(lambda x: reader(x))\ndataset9 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.ur7y-ziyb.csv\").mapPartitions(lambda x: reader(x))\ndataset10 \u003d sc.textFile(\"hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.esmb-8zkm.csv\").mapPartitions(lambda x: reader(x))"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset1.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset2.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset3.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset4.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset5.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset6.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset7.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset8.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset9.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(dataset10.first())"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nheader1 \u003d dataset1.first()\ndataset1 \u003d dataset1.filter(lambda line : line !\u003d header1)\nheader2 \u003d dataset2.first()\ndataset2 \u003d dataset2.filter(lambda line : line !\u003d header2)\nheader3 \u003d dataset3.first()\ndataset3 \u003d dataset3.filter(lambda line : line !\u003d header3)\nheader4 \u003d dataset4.first()\ndataset4 \u003d dataset4.filter(lambda line : line !\u003d header4)\nheader5 \u003d dataset5.first()\ndataset5 \u003d dataset5.filter(lambda line : line !\u003d header5)\nheader6 \u003d dataset6.first()\ndataset6 \u003d dataset6.filter(lambda line : line !\u003d header6)\nheader7 \u003d dataset7.first()\ndataset7 \u003d dataset7.filter(lambda line : line !\u003d header7)\nheader8 \u003d dataset8.first()\ndataset8 \u003d dataset8.filter(lambda line : line !\u003d header8)\nheader9 \u003d dataset9.first()\ndataset9 \u003d dataset9.filter(lambda line : line !\u003d header9)\nheader10 \u003d dataset10.first()\ndataset10 \u003d dataset10.filter(lambda line : line !\u003d header10)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough1 \u003d dataset1.map(lambda x : (x[7], 1)).countByKey()\nfor key, value in work_location_borough1.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough2 \u003d dataset2.map(lambda x : (x[2], 1)).countByKey()\nfor key, value in work_location_borough2.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough3 \u003d dataset3.map(lambda x : (x[4], 1)).countByKey()\nfor key, value in work_location_borough3.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough4 \u003d dataset4.map(lambda x : (x[0], 1)).countByKey()\nfor key, value in work_location_borough4.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough5 \u003d dataset5.map(lambda x : (x[1], 1)).countByKey()\nfor key, value in work_location_borough5.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough6 \u003d dataset6.map(lambda x : (x[1], 1)).countByKey()\nfor key, value in work_location_borough6.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough7 \u003d dataset7.map(lambda x : (x[1], 1)).countByKey()\nfor key, value in work_location_borough7.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough8 \u003d dataset8.map(lambda x : (x[1], 1)).countByKey()\nfor key, value in work_location_borough8.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough9 \u003d dataset9.map(lambda x : (x[1], 1)).countByKey()\nfor key, value in work_location_borough9.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwork_location_borough10 \u003d dataset10.map(lambda x : (x[1], 1)).countByKey()\nfor key, value in work_location_borough10.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndef clean(x):\n    x \u003d x.strip()\n    if x !\u003d \u0027\u0027:\n        return x.upper()\n    else:\n        return \"UNKNOWN\""
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds1 \u003d dataset1.map(lambda x: (clean(x[7]), 1)).countByKey()\nfor key, value in ds1.items():\n    print(key, value)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds2 \u003d dataset2.map(lambda x: (clean(x[2]), 1))\nfor key in ds2.items():\n    print(key)"
    }
  ]
}
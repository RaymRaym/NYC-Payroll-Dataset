{
  "metadata": {
    "name": "date-inital",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport datetime\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql.functions import to_date\nfrom pyspark.sql.functions import year, month, dayofmonth\nfrom csv import reader\nfrom operator import add"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# date"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# test pyspark to_date() function\ntest \u003d spark.createDataFrame([(\u002702-2-2020\u0027,)], [\u0027date\u0027])\ntest.select(to_date(test.date, \u0027MM-dd-yyyy\u0027).alias(\u0027date\u0027)).collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# NYPD arrest data\nds \u003d spark.read.csv(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.uip8-fykc.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\nds.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds \u003d ds.withColumn(\u0027ARREST_DATE\u0027, to_date(ds.ARREST_DATE, \u0027MM-dd-yyyy\u0027))\nds.printSchema()\nds.createOrReplaceTempView(\u0027ds\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select ARREST_KEY from ds where ARREST_DATE is null\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select current_date()\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select ARREST_KEY, ARREST_DATE from ds where ARREST_DATE\u003ecurrent_date()\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# NYC Payroll data\nds \u003d spark.read.csv(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.k397-673e.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\nds \u003d ds.withColumn(\u0027Agency Start Date\u0027, to_date(ds[\u0027Agency Start Date\u0027], \u0027MM/dd/yyyy\u0027))\nds.printSchema()\nds.createOrReplaceTempView(\u0027ds\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# check future dates\nspark.sql(\"select * from ds where `Agency Start Date`\u003ecurrent_date()\").show(50)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# name inital"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds \u003d spark.read.csv(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.w9ak-ipjd.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\nds.printSchema()\nds.createOrReplaceTempView(\"ds\")"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#`Applicants Middle Initial`\nspark.sql(\"select distinct `Applicants Middle Initial`, count(*) from ds group by `Applicants Middle Initial`\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nspark.sql(\"select distinct `Filing Representative Middle Initial`, count(*) from ds group by `Filing Representative Middle Initial`\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# DOB NOW: Build – Approved Permits\nds \u003d spark.read.csv(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.rbx6-tga4.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\nds.printSchema()\nds.createOrReplaceTempView(\"ds\")"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds.select(\u0027Applicant Middle Name\u0027).describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct `Applicant Middle Name` from ds\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds.select(\u0027Filing Representative Middle Initial\u0027).describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Greenbook\nds \u003d spark.read.csv(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.mdcw-n682.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\nds.printSchema()\nds.createOrReplaceTempView(\"ds\")"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds.select(\u0027Middle Initial\u0027).describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct `Middle Initial` from ds order by `Middle Initial`\").show(50)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Civil Service List (Active)\nds \u003d spark.read.csv(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.vx8i-nprf.csv\u0027, header\u003dTrue, inferSchema\u003dTrue)\nds \u003d ds.withColumn(\u0027Published Date\u0027, to_date(ds[\u0027Published Date\u0027], \u0027MM/dd/yyyy\u0027))\nds \u003d ds.withColumn(\u0027Established Date\u0027, to_date(ds[\u0027Established Date\u0027], \u0027MM/dd/yyyy\u0027))\nds \u003d ds.withColumn(\u0027Anniversary Date\u0027, to_date(ds[\u0027Anniversary Date\u0027], \u0027MM/dd/yyyy\u0027))\nds \u003d ds.withColumn(\u0027Extension Date\u0027, to_date(ds[\u0027Anniversary Date\u0027], \u0027MM/dd/yyyy\u0027))\n\nds.printSchema()\nds.createOrReplaceTempView(\u0027ds\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds.select(\u0027Extension Date\u0027).describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select count(*) from ds where `Extension Date`\u003ecurrent_date()\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds.select(\u0027MI\u0027).describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct MI from ds order by MI\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds \u003d ds.rdd\nprint(type(ds))"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.vx8i-nprf.csv\u0027).mapPartitions(lambda x : reader(x)).filter(lambda line : len(line)\u003e1 and \u0027MI\u0027 not in line)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndef edit(x):\n    if x[3].isupper():\n        return x\n    else:\n        x[3]\u003dNone\n        return x"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds \u003d ds.map(edit)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds.map(lambda x : x[3]).distinct().collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nds.first()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.sql(\"select distinct MI from ds order by MI\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# title description\n\n# Read All the data\n# NYC Civil Service Titles 1\nds1 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.nzjr-3966.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[1], 1]).filter(lambda line : len(line)\u003e1 and \u0027Title Description\u0027 not in line)\n# Civil Service Titles Subject to Investigation 1\nds2 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.h7fs-cqma.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[1], 1]).filter(lambda line : len(line)\u003e1 and \u0027TITLE DESCRIPTION\u0027 not in line)\n# Civil Service List Certification 7\nds3 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.a9md-ynri.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[7], 1]).filter(lambda line : len(line)\u003e1 and \u0027List Title Desc\u0027 not in line)\n# NYC Jobs 5\nds4 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.kpav-sd4t.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[5], 1]).filter(lambda line : len(line)\u003e1 and \u0027Civil Service Title\u0027 not in line)\n# Appeals Filed In 2017 7\nds5 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.7xq6-k6zy.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[7], 1]).filter(lambda line : len(line)\u003e1 and \u0027Title\u0027 not in line)\n# Appeals Filed In 2016 7\nds6 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.7h99-xsqt.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[7], 1]).filter(lambda line : len(line)\u003e1 and \u0027Title\u0027 not in line)\n# Appeals Filed In 2018 7\nds7 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.9vaq-mvvx.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[7], 1]).filter(lambda line : len(line)\u003e1 and \u0027Title\u0027 not in line)\n# Appeals Closed In 2018 7\nds8 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.uetw-jfrg.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[7], 1]).filter(lambda line : len(line)\u003e1 and \u0027Title\u0027 not in line)\n# Appeals Closed In 2016 7\nds9 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.wbtw-zkex.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[7], 1]).filter(lambda line : len(line)\u003e1 and \u0027Title\u0027 not in line)\n# Civil Service List (Terminated) 7\nds10 \u003d sc.textFile(\u0027hdfs:/user/CS-GY-6513/project_data/data-cityofnewyork-us.qu8g-sxqf.csv\u0027).mapPartitions(lambda x : reader(x)).map(lambda x: [x[7], 1]).filter(lambda line : len(line)\u003e1 and \u0027List Title Desc\u0027 not in line)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# ds1.reduceByKey(add).count()\n# ds2.reduceByKey(add).count()\n# ds3.reduceByKey(add).count()\n# ds4.reduceByKey(add).count()\n# ds5.reduceByKey(add).count()\n# ds6.reduceByKey(add).count()\n# ds7.reduceByKey(add).count()\n# ds8.reduceByKey(add).count()\n# ds9.reduceByKey(add).count()\n# ds10.reduceByKey(add).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nall \u003d ds1.union(ds2).union(ds3).union(ds4).union(ds5).union(ds6).union(ds7).union(ds8).union(ds9).union(ds10)\nall.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntitle \u003d all.reduceByKey(add)\ntitle.count()\n# 4140 -\u003e 2818"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntitle.first()"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nimport numpy as np\nimport sklearn.cluster\n\ndef LevenshteinDistance(s1, s2):\n    d \u003d [[x for x in range(len(s1)+1)] for _ in range(len(s2)+1)]\n    \n    for y in range(1,len(s2)+1):\n        d[y][0] \u003d d[y-1][0] + 1\n\n    for x in range(1, len(s1)+1):\n        for y in range(1, len(s2)+1):\n            if s1[x-1] \u003d\u003d s2[y-1]:\n                d[y][x] \u003d d[y-1][x-1]\n            else:\n                substute \u003d d[y-1][x-1] + 1\n                add \u003d d[y][x-1] + 1\n                delete \u003d d[y-1][x] + 1\n                d[y][x] \u003d min(add, substute, delete)\n    return d[-1][-1]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# words \u003d np.asarray([\u0027BOARD OF CORRECTIONS\u0027, \u0027BOARD OF CORRECTIOn\u0027,\u0027is\u0027,\u0027si\u0027]) #So that indexing with a list will work\ntitle \u003d title.collect()\nlev_similarity \u003d -1*np.array([[LevenshteinDistance(t1,t2) for t1 in title] for t2 in title])\nprint(lev_similarity)\n\naffprop \u003d sklearn.cluster.AffinityPropagation(affinity\u003d\"precomputed\", damping\u003d0.5)\naffprop.fit(lev_similarity)\n# for cluster_id in np.unique(affprop.labels_):\n#     exemplar \u003d title[affprop.cluster_centers_indices_[cluster_id]]\n#     cluster \u003d np.unique(title[np.nonzero(affprop.labels_\u003d\u003dcluster_id)])\n#     cluster_str \u003d \", \".join(cluster)\n#     print(\" - *%s:* %s\" % (exemplar, cluster_str))"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nres \u003d all.countByKey()\nfor key, value in res.items():\n    print(key, value)"
    }
  ]
}